{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "Fetch Title, Publish date, Likes and Comments for a blog.\n",
    "\n",
    "1. Get all links for the blog posts\n",
    "2. Get titles\n",
    "3. Get author\n",
    "4. Get publishing date\n",
    "5. Get num of likes and shares\n",
    "6. Get num of comments on each post\n",
    "7. Get contents of comments\n",
    "8. Store all the above in a data frame (Using pandas ?)\n",
    "\n",
    "The following code fetches/scrapes the necessary data as mentioned above with the help of\n",
    "a library named BeautifulSoup. First we try to find out how many pages are there in the blog.\n",
    "Then we query these pages to find out corresponding link to the posts.\n",
    "These links help us get data for each post in a single loop.\n",
    "The data found is written into lists which are then combined to form a data frame.\n",
    "This data frame is subsequently written to an excel sheet.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Import all necessary libraries\n",
    "import requests\n",
    "import bs4 \n",
    "import urllib2\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "url = 'https://textnook.wordpress.com/page/'     # Main URL of the page\n",
    "page_count = 1                                   # Variable to hold the number of pages in the blog\n",
    "links = []                                       # A list to store all main page links\n",
    "while True:\n",
    "    \n",
    "    try:\n",
    "        # Check if link is valid\n",
    "        urllib2.urlopen(url+str(page_count))\n",
    "           \n",
    "    except Exception:\n",
    "        # If link is invalid break the loop and decrease page count as the current page doesnt exist\n",
    "        page_count-=1\n",
    "        break\n",
    "       \n",
    "    links.append(url+str(page_count))           # Add the links to a list\n",
    "    page_count+=1                               # Increase page count\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "blog_links=[]                                  # List to store links of individual posts\n",
    "for i in range(page_count):\n",
    "\n",
    "    link = urllib2.urlopen(links[i])\n",
    "\n",
    "    soup = bs4.BeautifulSoup(link, from_encoding=\"utf-8\")\n",
    "\n",
    "    blog_links.append(soup.findAll('h1', class_='entry-title'))\n",
    "    \n",
    "blog_links=[l for subl in blog_links for l in subl]\n",
    "\n",
    "\n",
    "\n",
    "# Lists to hold the necessary data\n",
    "titles = []          # Store the post title in a list\n",
    "post_date = []       # Store post date in a list\n",
    "post_author = []     # Store author name in a list\n",
    "post_category = []   # Store category in a list\n",
    "post_comments = []   # Store comments on the post in a list\n",
    "# post_likes = []\n",
    "for i in range(len(blog_links)):\n",
    "    titles.append(blog_links[i].getText())                                          \n",
    "    link = requests.get(blog_links[i].find('a').get('href'))\n",
    "    soup = bs4.BeautifulSoup(link.text, from_encoding=\"utf-8\")\n",
    "    post_date.append(soup.find('time').getText())                                   \n",
    "    post_author.append(soup.find('span', class_='author vcard').getText())            \n",
    "    post_category.append(soup.find('span', class_='cats-links').find('a').getText())  \n",
    "\n",
    "    # Find all comments on a blog post and save to a list and if no comments append None\n",
    "    \n",
    "    if soup.find_all('div',{'class':'comment-content'}) != []:\n",
    "        comments = soup.findAll('div', {'class': 'comment-content'})\n",
    "        post_comments.append(map(str, (comments[i].getText().strip('\\n') \n",
    "                                       for i in range(len(comments)))))\n",
    "\n",
    "    else:\n",
    "        post_comments.append('None')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Convert the lists into a data frame\n",
    "df = pd.DataFrame([titles, post_author, post_date, post_category, post_comments]).T\n",
    "\n",
    "\n",
    "\n",
    "# Write the data frame to an excel sheet\n",
    "df.columns=['Post Title', 'Post Author', 'Post Date', 'Post Category', 'Comments']\n",
    "\n",
    "\n",
    "df.to_excel('Blog Details.xls', encoding='utf-8', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
